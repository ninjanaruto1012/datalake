{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b17dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/22 21:03:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from os.path import abspath\n",
    "\n",
    "# SparkSession\n",
    "URL_SPARK = \"spark://spark-master:7077\"\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"write-stream-elab9070-minio\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.cores.max\", \"2\")\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location)\n",
    "    .config(\"spark.jars\", \"jars/hadoop-aws-3.3.2.jar,jars/aws-java-sdk-bundle-1.12.172.jar,jars/hadoop-common-3.3.2.jar,jars/spark-avro_2.12-3.3.2.jar,jars/commons-pool2-2.11.1.jar,jars/spark-sql-kafka-0-10_2.12-3.3.2.jar,jars/kafka-clients-2.8.1.jar,jars/spark-streaming-kafka-0-10-assembly_2.12-3.2.1.jar\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .master(URL_SPARK)\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26f45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "# os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://sparkjupyter-minio-1:9000\"\n",
    "\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://sparkjupyter-minio-1:9000\")\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minioadmin\")\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minioadmin\" )\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\",\"false\")\n",
    "import os\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minio_access_key\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minio_secret_key\"\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = f\"http://minio:9000\"\n",
    "\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio_access_key\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minio_secret_key\" )\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54ed824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f9afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"10.133.100.199:9092\") \\\n",
    "            .option(\"subscribe\", \"telegraf1\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load()\n",
    "stringDF = df.selectExpr(\"CAST(timestamp AS STRING)\",\"CAST(value AS STRING)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dad7800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn('timestamp', regexp_extract('timestamp', r'last_data_time=([^\\Wi]+)', 1))\\\n",
    "df_generic_counter = stringDF.withColumn('source', regexp_extract('value', r'source=(.*?),', 1))\\\n",
    "        .withColumn('interface_name', regexp_extract('value', r'interface_name=(.*?),', 1))\\\n",
    "        .withColumn('packets_received', regexp_extract('value', r'packets_received=([^\\Wi,bytes_received]+)', 1))\\\n",
    "        .withColumn('packets_sent', regexp_extract('value', r'packets_sent=([^\\Wi]+)', 1)) \\\n",
    "        .withColumn('bytes_received', regexp_extract('value', r'bytes_received=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('bytes_sent', regexp_extract('value', r'bytes_sent=([^\\Wi]+)', 1)) \\\n",
    "        .withColumn('multicast_packets_received', regexp_extract('value', r'multicast_packets_received=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('multicast_packets_sent',regexp_extract('value', r'multicast_packets_sent=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('broadcast_packets_received', regexp_extract('value', r'broadcast_packets_received=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('broadcast_packets_sent',regexp_extract('value', r'broadcast_packets_sent=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('output_drops',regexp_extract('value', r'output_drops=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('output_queue_drops',regexp_extract('value', r'output_queue_drops=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('input_drops',regexp_extract('value', r'input_drops=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('input_queue_drops',regexp_extract('value', r'input_queue_drops=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('input_errors',regexp_extract('value', r'input_errors=([^\\Wi]+)', 1))\\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04990c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generic_counter = df_generic_counter.drop('value')\n",
    "df_generic_counter = df_generic_counter.filter(df_generic_counter.interface_name!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4f267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_generic_counter = df_generic_counter.withColumn('timestampType',F.from_unixtime(F.col('timestamp')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4280532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_generic_counter.writeStream.format('console').start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd0f3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CPU = stringDF.withColumn('node', regexp_extract('value', r'node_name=(.*?),', 1))\\\n",
    "        .withColumn('source', regexp_extract('value', r'source=(.*?),', 1))\\\n",
    "        .withColumn('process_name', regexp_extract('value', r'process_name=(.*?),', 1))\\\n",
    "        .withColumn('cpu_1min', regexp_extract('value', r'Sub2 total_cpu_one_minute=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('cpu_5min', regexp_extract('value', r'total_cpu_five_minute=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('cpu_15min',regexp_extract('value', r'total_cpu_fifteen_minute=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('process_id',regexp_extract('value', r'process_id=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('process_cpu_1min',regexp_extract('value', r'process_cpu_one_minute=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('process_cpu_5min',regexp_extract('value', r'process_cpu_five_minute=([^\\Wi]+)', 1))\\\n",
    "        .withColumn('process_cpu_15min',regexp_extract('value', r'process_cpu_fifteen_minute=(.*?)i', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a843cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CPU = df_CPU.withColumn('timestamp',regexp_extract('process_cpu_15min', r'i\\s(.*?)$', 1))\\\n",
    "#             .withColumn('process_cpu_15min_rm',regexp_extract('process_cpu_15min', r'(.*?)i', 1))\\\n",
    "#             .drop('value')\\\n",
    "#             .drop('process_cpu_15min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d21961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_CPU = df_CPU.withColumnRenamed('process_cpu_15min_rm','process_cpu_15min')\n",
    "df_CPU = df_CPU.drop('value')\n",
    "df_CPU = df_CPU.filter(df_CPU.node!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6625c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_generic_counter.writeStream.format('console').start()\n",
    "# df_CPU.writeStream.format('console').option(\"numRows\",60).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97a0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/22 21:03:37 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/10/22 21:03:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5ef50838b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_CPU.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"s3a://stream-cpu-elab9070/data\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://stream-cpu-elab9070/checkpoint\") \\\n",
    "    .start()\n",
    "#     .toTable(\"Table_cpu_Elab9070\")\n",
    "\n",
    "# df_CPU.write.option(\"path\",\"s3a://stream-cpu-elab9070/mytable\").saveAsTable(\"default.testcpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58923a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/22 21:03:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5ef50830d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_generic_counter.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"path\", \"s3a://stream-interface-elab9070/data\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://stream-interface-elab9070/checkpoint\") \\\n",
    "    .start()\n",
    "# #     .toTable(\"Table_interface_Elab9070\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d531ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method StreamingQueryManager.awaitAnyTermination of <pyspark.sql.streaming.StreamingQueryManager object at 0x7f5ec6f82340>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/22 21:04:46 WARN FileStreamSinkLog: Compacting took 2498 ms for compact batch 23159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/22 21:29:56 WARN FileStreamSinkLog: Compacting took 2032 ms for compact batch 23189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 01:16:08 WARN FileStreamSinkLog: Compacting took 2026 ms for compact batch 23339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 08:17:52 WARN FileStreamSinkLog: Compacting took 2026 ms for compact batch 23849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 08:48:52 WARN FileStreamSinkLog: Compacting took 2078 ms for compact batch 24019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 11:33:03 WARN FileStreamSinkLog: Compacting took 2015 ms for compact batch 24229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 12:13:04 WARN FileStreamSinkLog: Compacting took 2018 ms for compact batch 24279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 15:05:25 WARN FileStreamSinkLog: Compacting took 2064 ms for compact batch 24509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 15:09:26 WARN FileStreamSinkLog: Compacting took 2049 ms for compact batch 24389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 16:11:26 WARN FileStreamSinkLog: Compacting took 2093 ms for compact batch 24589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 16:35:26 WARN FileStreamSinkLog: Compacting took 2039 ms for compact batch 24619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 17:05:37 WARN FileStreamSinkLog: Compacting took 2040 ms for compact batch 24659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 17:38:37 WARN FileStreamSinkLog: Compacting took 2211 ms for compact batch 24569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 17:45:37 WARN FileStreamSinkLog: Compacting took 2009 ms for compact batch 24579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 18:02:37 WARN FileStreamSinkLog: Compacting took 2013 ms for compact batch 24599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 19:57:48 WARN FileStreamSinkLog: Compacting took 2018 ms for compact batch 24869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 20:18:48 WARN FileStreamSinkLog: Compacting took 2040 ms for compact batch 24769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 20:55:49 WARN FileStreamSinkLog: Compacting took 2021 ms for compact batch 24939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 21:10:49 WARN FileStreamSinkLog: Compacting took 2129 ms for compact batch 24959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/10/23 21:46:59 WARN FileStreamSinkLog: Compacting took 2039 ms for compact batch 25009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.streams.awaitAnyTermination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04cb93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
